{"cells":[{"cell_type":"markdown","metadata":{"id":"yCF6eWlQGjH5"},"source":["## BERT 구현 과정 (1/2)\n","![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2020-01-02/bert-pretrain.png)\n","\n","BERT 모델 구현에 대한 설명 입니다.\n","\n","이 내용을 확인하기 전 아래 내용을 확인하시기 바랍니다.\n","- [Sentencepiece를 활용해 Vocab 만들기](https://paul-hyun.github.io/vocab-with-sentencepiece/)\n","- [Naver 영화리뷰 감정분석 데이터 전처리 하기](https://paul-hyun.github.io/preprocess-nsmc/)\n","- [Transformer (Attention Is All You Need) 구현하기 (1/3)](https://paul-hyun.github.io/transformer-01/)\n","- [Transformer (Attention Is All You Need) 구현하기 (2/3)](https://paul-hyun.github.io/transformer-02/)\n","- [Transformer (Attention Is All You Need) 구현하기 (3/3)](https://paul-hyun.github.io/transformer-03/)\n","\n","[Colab](https://colab.research.google.com/)에서 실행 했습니다."]},{"cell_type":"markdown","metadata":{"id":"YcKmueewQQbf"},"source":["#### 0. Pip Install\n","필요한 패키지를 pip를 이용해서 설치합니다."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15667,"status":"ok","timestamp":1701410388689,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"2B9WSoPaG5va","outputId":"02b7c2e9-ee3c-4835-caf5-2171a989dc4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=471f202b1ff0ce58ecdb31214a38692cb4b22c7fde61f43937ca4e3e4b9ea9ad\n","  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}],"source":["!pip install sentencepiece\n","!pip install wget"]},{"cell_type":"markdown","metadata":{"id":"7t57ZDrpQVPf"},"source":["#### 1. Google Drive Mount\n","Colab에서는 컴퓨터에 자원에 접근이 불가능 하므로 Google Drive에 파일을 올려 놓은 후 Google Drive를 mount 에서 로컬 디스크처럼 사용 합니다.\n","1. 아래 블럭을 실행하면 나타나는 링크를 클릭하세요.\n","2. Google 계정을 선택 하시고 허용을 누르면 나타나는 코드를 복사하여 아래 박스에 입력한 후 Enter 키를 입력하면 됩니다.\n","\n","학습관련 [데이터 및 결과 파일](https://drive.google.com/open?id=15XGr-L-W6DSoR5TbniPMJASPsA0IDTiN)을 참고 하세요."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16716,"status":"ok","timestamp":1701410405401,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"lo3kfdTxM_dx","outputId":"c22d3ed0-a2e9-4262-b7fc-26c0eef5f1b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# data를 저장할 폴더 입니다. 환경에 맞게 수정 하세요.\n","data_dir = \"/content/drive/MyDrive/dataset/Text\""]},{"cell_type":"markdown","metadata":{"id":"5K0u3OtaQcCG"},"source":["#### 2. Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5286,"status":"ok","timestamp":1701410410685,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"uqJFoQFbQfaX"},"outputs":[],"source":["import os\n","import numpy as np\n","import math\n","from random import random, randrange, randint, shuffle, choice\n","import matplotlib.pyplot as plt\n","import json\n","import pandas as pd\n","from IPython.display import display\n","from tqdm import tqdm, tqdm_notebook, trange\n","import sentencepiece as spm\n","import wget\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{"id":"VuQJkP4FQ17R"},"source":["#### 3. 폴더의 목록을 확인\n","Google Drive mount가 잘 되었는지 확인하기 위해 data_dir 목록을 확인 합니다."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":458,"status":"ok","timestamp":1701410411141,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"sw9syi7mQ4Wf","outputId":"818689a8-a269-4fac-a116-fd40beaa5f20"},"outputs":[{"output_type":"stream","name":"stdout","text":["ratings_train.txt\n","naver_review.vocab\n","naver_review.model\n","ratings_train_bert_0.json\n"]}],"source":["for f in os.listdir(data_dir):\n","  print(f)"]},{"cell_type":"markdown","metadata":{"id":"1TEqHwEVQ7ch"},"source":["#### 4. Vocab 및 입력\n","[Sentencepiece를 활용해 Vocab 만들기](https://paul-hyun.github.io/vocab-with-sentencepiece/)를 통해 만들어 놓은 vocab을 로딩 합니다."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1701410411141,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"lJk2o8ykRDNn","outputId":"43c09936-661c-4b02-cfe2-955b7e336842"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}],"source":["# vocab loading\n","vocab_file = f\"{data_dir}/naver_review.model\"\n","vocab = spm.SentencePieceProcessor()\n","vocab.load(vocab_file)"]},{"cell_type":"markdown","metadata":{"id":"cepm-CRzQ-Jf"},"source":["#### 5. Config\n","모델에 설정 값을 전달하기 위한 config를 만듭니다."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701410411141,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"Az1JWouiRAgP"},"outputs":[],"source":["\"\"\" configuration json을 읽어들이는 class \"\"\"\n","class Config(dict):\n","    __getattr__ = dict.__getitem__\n","    __setattr__ = dict.__setitem__\n","\n","    @classmethod\n","    def load(cls, file):\n","        with open(file, 'r') as f:\n","            config = json.loads(f.read())\n","            return Config(config)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701410411141,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"PUBr9IYkRH8v","outputId":"10ca02b6-6938-4421-bf21-66dfb4480cc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'n_enc_vocab': 8007, 'n_enc_seq': 256, 'n_seg_type': 2, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"]}],"source":["config = Config({\n","    \"n_enc_vocab\": len(vocab),\n","    \"n_enc_seq\": 256,\n","    \"n_seg_type\": 2,\n","    \"n_layer\": 6,\n","    \"d_hidn\": 256,\n","    \"i_pad\": 0,\n","    \"d_ff\": 1024,\n","    \"n_head\": 4,\n","    \"d_head\": 64,\n","    \"dropout\": 0.1,\n","    \"layer_norm_epsilon\": 1e-12\n","})\n","print(config)"]},{"cell_type":"markdown","metadata":{"id":"yavmvonqRL5g"},"source":["#### 6. Common Class\n","공통으로 사용되는 Class 및 함수 입니다."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701410411141,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"lY99m9lMRMy2"},"outputs":[],"source":["\"\"\" sinusoid position encoding \"\"\"\n","def get_sinusoid_encoding_table(n_seq, d_hidn):\n","    def cal_angle(position, i_hidn):\n","        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\n","    def get_posi_angle_vec(position):\n","        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\n","\n","    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin\n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\n","\n","    return sinusoid_table\n","\n","\n","\"\"\" attention pad mask \"\"\"\n","def get_attn_pad_mask(seq_q, seq_k, i_pad):\n","    batch_size, len_q = seq_q.size()\n","    batch_size, len_k = seq_k.size()\n","    pad_attn_mask = seq_k.data.eq(i_pad).unsqueeze(1).expand(batch_size, len_q, len_k)  # <pad>\n","    return pad_attn_mask\n","\n","\n","\"\"\" attention decoder mask \"\"\"\n","def get_attn_decoder_mask(seq):\n","    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n","    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n","    return subsequent_mask\n","\n","\n","\"\"\" scale dot product attention \"\"\"\n","class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.scale = 1 / (self.config.d_head ** 0.5)\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        # (bs, n_head, n_q_seq, n_k_seq)\n","        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\n","        scores.masked_fill_(attn_mask, -1e9)\n","        # (bs, n_head, n_q_seq, n_k_seq)\n","        attn_prob = nn.Softmax(dim=-1)(scores)\n","        attn_prob = self.dropout(attn_prob)\n","        # (bs, n_head, n_q_seq, d_v)\n","        context = torch.matmul(attn_prob, V)\n","        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n","        return context, attn_prob\n","\n","\n","\"\"\" multi head attention \"\"\"\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n","        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n","        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n","        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n","        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        batch_size = Q.size(0)\n","        # (bs, n_head, n_q_seq, d_head)\n","        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n","        # (bs, n_head, n_k_seq, d_head)\n","        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n","        # (bs, n_head, n_v_seq, d_head)\n","        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n","\n","        # (bs, n_head, n_q_seq, n_k_seq)\n","        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n","\n","        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n","        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n","        # (bs, n_head, n_q_seq, h_head * d_head)\n","        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n","        # (bs, n_head, n_q_seq, e_embd)\n","        output = self.linear(context)\n","        output = self.dropout(output)\n","        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n","        return output, attn_prob\n","\n","\n","\"\"\" feed forward \"\"\"\n","class PoswiseFeedForwardNet(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n","        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n","        self.active = F.gelu\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","    def forward(self, inputs):\n","        # (bs, d_ff, n_seq)\n","        output = self.active(self.conv1(inputs.transpose(1, 2)))\n","        # (bs, n_seq, d_hidn)\n","        output = self.conv2(output).transpose(1, 2)\n","        output = self.dropout(output)\n","        # (bs, n_seq, d_hidn)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"9rmgSjxPRPzK"},"source":["#### 7. Encoder\n","Encoder 입니다.\n","\n","표준 Transformer Encoder에서 BERT에서 추가된 정의한 segment embedding만 추가 합니다."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701410411141,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"qtfrgJ9ORcwI"},"outputs":[],"source":["\"\"\" encoder layer \"\"\"\n","class EncoderLayer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.self_attn = MultiHeadAttention(self.config)\n","        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n","        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n","        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n","\n","    def forward(self, inputs, attn_mask):\n","        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n","        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n","        att_outputs = self.layer_norm1(inputs + att_outputs)\n","        # (bs, n_enc_seq, d_hidn)\n","        ffn_outputs = self.pos_ffn(att_outputs)\n","        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\n","        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n","        return ffn_outputs, attn_prob"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701410411141,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"fIrkxJ-SRrxq"},"outputs":[],"source":["\"\"\" encoder \"\"\"\n","class Encoder(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n","        self.pos_emb = nn.Embedding(self.config.n_enc_seq + 1, self.config.d_hidn)\n","        self.seg_emb = nn.Embedding(self.config.n_seg_type, self.config.d_hidn)\n","\n","        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n","\n","    def forward(self, inputs, segments):\n","        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n","        pos_mask = inputs.eq(self.config.i_pad)\n","        positions.masked_fill_(pos_mask, 0)\n","\n","        # (bs, n_enc_seq, d_hidn)\n","        outputs = self.enc_emb(inputs) + self.pos_emb(positions)  + self.seg_emb(segments)\n","\n","        # (bs, n_enc_seq, n_enc_seq)\n","        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n","\n","        attn_probs = []\n","        for layer in self.layers:\n","            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n","            outputs, attn_prob = layer(outputs, attn_mask)\n","            attn_probs.append(attn_prob)\n","        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n","        return outputs, attn_probs"]},{"cell_type":"markdown","metadata":{"id":"cexyfhsQSHwK"},"source":["#### 8. BERT\n","BERT 입니다.\n","\n","Transformer Encoder를 실행 합니다.  \n","이후 첫번째 토큰을 linear, tahn를 실행 합니다."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":297,"status":"ok","timestamp":1701410411436,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"tVOKzzvKSNjc"},"outputs":[],"source":["\"\"\" bert \"\"\"\n","class BERT(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.encoder = Encoder(self.config)\n","\n","        self.linear = nn.Linear(config.d_hidn, config.d_hidn)\n","        self.activation = torch.tanh\n","\n","    def forward(self, inputs, segments):\n","        # (bs, n_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n","        outputs, self_attn_probs = self.encoder(inputs, segments)\n","        # (bs, d_hidn)\n","        outputs_cls = outputs[:, 0].contiguous()\n","        outputs_cls = self.linear(outputs_cls)\n","        outputs_cls = self.activation(outputs_cls)\n","        # (bs, n_enc_seq, n_enc_vocab), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n","        return outputs, outputs_cls, self_attn_probs\n","\n","    def save(self, epoch, loss, path):\n","        torch.save({\n","            \"epoch\": epoch,\n","            \"loss\": loss,\n","            \"state_dict\": self.state_dict()\n","        }, path)\n","\n","    def load(self, path):\n","        save = torch.load(path)\n","        self.load_state_dict(save[\"state_dict\"])\n","        return save[\"epoch\"], save[\"loss\"]"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701410411436,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"JM9YREyBSU-y"},"outputs":[],"source":["\"\"\" BERT pretrain \"\"\"\n","class BERTPretrain(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        self.bert = BERT(self.config)\n","        # classfier\n","        self.projection_cls = nn.Linear(self.config.d_hidn, 2, bias=False)\n","        # lm\n","        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_enc_vocab, bias=False)\n","        self.projection_lm.weight = self.bert.encoder.enc_emb.weight\n","\n","    def forward(self, inputs, segments):\n","        # (bs, n_enc_seq, d_hidn), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n","        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n","        # (bs, 2)\n","        logits_cls = self.projection_cls(outputs_cls)\n","        # (bs, n_enc_seq, n_enc_vocab)\n","        logits_lm = self.projection_lm(outputs)\n","        # (bs, n_enc_vocab), (bs, n_enc_seq, n_enc_vocab), [(bs, n_head, n_enc_seq, n_enc_seq)]\n","        return logits_cls, logits_lm, attn_probs"]},{"cell_type":"markdown","metadata":{"id":"0411eNM9Sb1s"},"source":["#### 9. Pretrain Data 준비"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701410411436,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"JCjhydEqSc1x"},"outputs":[],"source":["\"\"\" 마스크 생성 \"\"\"\n","def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n","    cand_idx = []\n","    for (i, token) in enumerate(tokens):\n","        if token == \"[CLS]\" or token == \"[SEP]\":\n","            continue\n","        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n","            cand_idx[-1].append(i)\n","        else:\n","            cand_idx.append([i])\n","    shuffle(cand_idx)\n","\n","    mask_lms = []\n","    for index_set in cand_idx:\n","        if len(mask_lms) >= mask_cnt:\n","            break\n","        if len(mask_lms) + len(index_set) > mask_cnt:\n","            continue\n","        for index in index_set:\n","            masked_token = None\n","            if random() < 0.8: # 80% replace with [MASK]\n","                masked_token = \"[MASK]\"\n","            else:\n","                if random() < 0.5: # 10% keep original\n","                    masked_token = tokens[index]\n","                else: # 10% random word\n","                    masked_token = choice(vocab_list)\n","            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n","            tokens[index] = masked_token\n","    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n","    mask_idx = [p[\"index\"] for p in mask_lms]\n","    mask_label = [p[\"label\"] for p in mask_lms]\n","\n","    return tokens, mask_idx, mask_label"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701410411436,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"Dryud7_9Shvz"},"outputs":[],"source":["\"\"\" 쵀대 길이 초과하는 토큰 자르기 \"\"\"\n","def trim_tokens(tokens_a, tokens_b, max_seq):\n","    while True:\n","        total_length = len(tokens_a) + len(tokens_b)\n","        if total_length <= max_seq:\n","            break\n","\n","        if len(tokens_a) > len(tokens_b):\n","            del tokens_a[0]\n","        else:\n","            tokens_b.pop()"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701410411436,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"9sCSQ0a6Sj-i"},"outputs":[],"source":["\"\"\" doc별 pretrain 데이터 생성 \"\"\"\n","def create_pretrain_instances(docs, doc_idx, doc, n_seq, mask_prob, vocab_list):\n","    # for CLS], [SEP], [SEP]\n","    max_seq = n_seq - 3\n","    tgt_seq = max_seq\n","\n","    instances = []\n","    current_chunk = []\n","    current_length = 0\n","    for i in range(len(doc)):\n","        current_chunk.append(doc[i]) # line\n","        current_length += len(doc[i])\n","        if i == len(doc) - 1 or current_length >= tgt_seq:\n","            if 0 < len(current_chunk):\n","                a_end = 1\n","                if 1 < len(current_chunk):\n","                    a_end = randrange(1, len(current_chunk))\n","                tokens_a = []\n","                for j in range(a_end):\n","                    tokens_a.extend(current_chunk[j])\n","\n","                tokens_b = []\n","                if len(current_chunk) == 1 or random() < 0.5:\n","                    is_next = 0\n","                    tokens_b_len = tgt_seq - len(tokens_a)\n","                    random_doc_idx = doc_idx\n","                    while doc_idx == random_doc_idx:\n","                        random_doc_idx = randrange(0, len(docs))\n","                    random_doc = docs[random_doc_idx]\n","\n","                    random_start = randrange(0, len(random_doc))\n","                    for j in range(random_start, len(random_doc)):\n","                        tokens_b.extend(random_doc[j])\n","                else:\n","                    is_next = 1\n","                    for j in range(a_end, len(current_chunk)):\n","                        tokens_b.extend(current_chunk[j])\n","\n","                trim_tokens(tokens_a, tokens_b, max_seq)\n","                assert 0 < len(tokens_a)\n","                assert 0 < len(tokens_b)\n","\n","                tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n","                segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n","\n","                tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n","\n","                instance = {\n","                    \"tokens\": tokens,\n","                    \"segment\": segment,\n","                    \"is_next\": is_next,\n","                    \"mask_idx\": mask_idx,\n","                    \"mask_label\": mask_label\n","                }\n","                instances.append(instance)\n","\n","            current_chunk = []\n","            current_length = 0\n","    return instances"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1701410411437,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"},"user_tz":-540},"id":"TY-6jSDsSuvd"},"outputs":[],"source":["\"\"\" pretrain 데이터 생성 \"\"\"\n","def make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob):\n","    vocab_list = []\n","    for id in range(vocab.get_piece_size()):\n","        if not vocab.is_unknown(id):\n","            vocab_list.append(vocab.id_to_piece(id))\n","\n","    line_cnt = 0\n","    with open(in_file, \"r\") as in_f:\n","        for line in in_f:\n","            line_cnt += 1\n","\n","    docs = []\n","    with open(in_file, \"r\") as f:\n","        doc = []\n","        with tqdm_notebook(total=line_cnt, desc=f\"Loading\") as pbar:\n","            for i, line in enumerate(f):\n","                line = line.strip()\n","                if line == \"\":\n","                    if 0 < len(doc):\n","                        docs.append(doc)\n","                        doc = []\n","                        # 메모리 사용량을 줄이기 위해 100,000개만 처리 함\n","                        if 100000 < len(docs): break\n","                else:\n","                    pieces = vocab.encode_as_pieces(line)\n","                    if 0 < len(pieces):\n","                        doc.append(pieces)\n","                pbar.update(1)\n","        if doc:\n","            docs.append(doc)\n","\n","    for index in range(count):\n","        output = out_file.format(index)\n","        if os.path.isfile(output): continue\n","\n","        with open(output, \"w\") as out_f:\n","            with tqdm_notebook(total=len(docs), desc=f\"Making\") as pbar:\n","                for i, doc in enumerate(docs):\n","                    instances = create_pretrain_instances(docs, i, doc, n_seq, mask_prob, vocab_list)\n","                    for instance in instances:\n","                        out_f.write(json.dumps(instance))\n","                        out_f.write(\"\\n\")\n","                    pbar.update(1)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["695d036ac30e4713b0e530e7c33cdf4d","04941caeb4f347d5b2c97baa3f9f3c56","135656ae3df74edcabb1671f893c1a36","29254ad115fc452982088336d4d6d751","03c74b5cd7974f0f8b8ccca5c18d96c2","ae126c9cc52f48318ea6039bccadfb0b","6c6452b3781a43fca41d113b9a6b8bf9","fd78b5ff04b6414ca2847570f94d712a","93323cd3f75a4c4693af4add00b5aab0","097d23f411844023b7ec1a000c989d5d","47b06f4dda5241b8ae164d993f2cf882"]},"id":"7_ihvSolSzoT","executionInfo":{"status":"ok","timestamp":1701410429084,"user_tz":-540,"elapsed":17650,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"947cb796-461c-492f-bc7c-ac46c231c888"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-e8eda6bf5cff>:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  with tqdm_notebook(total=line_cnt, desc=f\"Loading\") as pbar:\n"]},{"output_type":"display_data","data":{"text/plain":["Loading:   0%|          | 0/150001 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"695d036ac30e4713b0e530e7c33cdf4d"}},"metadata":{}}],"source":["in_file = f\"{data_dir}/ratings_train.txt\"\n","out_file = f\"{data_dir}/ratings_train_bert\" + \"_{}.json\"\n","count = 1\n","n_seq = 256\n","mask_prob = 0.15\n","\n","make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob)"]},{"cell_type":"markdown","metadata":{"id":"1WH8ygciLOsg"},"source":["#### 10. Pretrain Data\n","GPT Pretrain Data 입니다."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"wYQy0jh7LPlM","executionInfo":{"status":"ok","timestamp":1701410429084,"user_tz":-540,"elapsed":16,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[],"source":["\"\"\" pretrain 데이터셋 \"\"\"\n","class PretrainDataSet(torch.utils.data.Dataset):\n","    def __init__(self, vocab, infile):\n","        self.vocab = vocab\n","        self.labels_cls = []\n","        self.labels_lm = []\n","        self.sentences = []\n","        self.segments = []\n","\n","        line_cnt = 0\n","        with open(infile, \"r\") as f:\n","            for line in f:\n","                line_cnt += 1\n","\n","        with open(infile, \"r\") as f:\n","            for i, line in enumerate(tqdm(f, total=line_cnt, desc=f\"Loading {infile}\", unit=\" lines\")):\n","                instance = json.loads(line)\n","                self.labels_cls.append(instance[\"is_next\"])\n","                sentences = [vocab.piece_to_id(p) for p in instance[\"tokens\"]]\n","                self.sentences.append(sentences)\n","                self.segments.append(instance[\"segment\"])\n","                mask_idx = np.array(instance[\"mask_idx\"], dtype=np.int)\n","                mask_label = np.array([vocab.piece_to_id(p) for p in instance[\"mask_label\"]], dtype=np.int)\n","                label_lm = np.full(len(sentences), dtype=np.int, fill_value=-1)\n","                label_lm[mask_idx] = mask_label\n","                self.labels_lm.append(label_lm)\n","\n","    def __len__(self):\n","        assert len(self.labels_cls) == len(self.labels_lm)\n","        assert len(self.labels_cls) == len(self.sentences)\n","        assert len(self.labels_cls) == len(self.segments)\n","        return len(self.labels_cls)\n","\n","    def __getitem__(self, item):\n","        return (torch.tensor(self.labels_cls[item]),\n","                torch.tensor(self.labels_lm[item]),\n","                torch.tensor(self.sentences[item]),\n","                torch.tensor(self.segments[item]))"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"fPEpzcFmLZ-7","executionInfo":{"status":"ok","timestamp":1701410429084,"user_tz":-540,"elapsed":15,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[],"source":["\"\"\" pretrain data collate_fn \"\"\"\n","def pretrin_collate_fn(inputs):\n","    labels_cls, labels_lm, inputs, segments = list(zip(*inputs))\n","\n","    labels_lm = torch.nn.utils.rnn.pad_sequence(labels_lm, batch_first=True, padding_value=-1)\n","    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n","    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n","\n","    batch = [\n","        torch.stack(labels_cls, dim=0),\n","        labels_lm,\n","        inputs,\n","        segments\n","    ]\n","    return batch"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"Z7wrvZVOLca8","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"error","timestamp":1701410429498,"user_tz":-540,"elapsed":6,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"ba536141-90eb-4fe7-dc12-be562b380f63"},"outputs":[{"output_type":"stream","name":"stderr","text":["Loading /content/drive/MyDrive/dataset/Text/ratings_train_bert_0.json: 0 lines [00:00, ? lines/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-0fd12fbb9c3e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{data_dir}/ratings_train_bert_0.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrin_collate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"]}],"source":["\"\"\" pretrain 데이터 로더 \"\"\"\n","batch_size = 128\n","dataset = PretrainDataSet(vocab, f\"{data_dir}/ratings_train_bert_0.json\")\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pretrin_collate_fn)"]},{"cell_type":"markdown","metadata":{"id":"yERFmhF-LmKM"},"source":["#### 11. Pretrain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bN9IHtGBLm5z","executionInfo":{"status":"aborted","timestamp":1701410429498,"user_tz":-540,"elapsed":3,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[],"source":["\"\"\" 모델 epoch 학습 \"\"\"\n","def train_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader):\n","    losses = []\n","    model.train()\n","\n","    with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n","        for i, value in enumerate(train_loader):\n","            labels_cls, labels_lm, inputs, segments = map(lambda v: v.to(config.device), value)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs, segments)\n","            logits_cls, logits_lm = outputs[0], outputs[1]\n","\n","            loss_cls = criterion_cls(logits_cls, labels_cls)\n","            loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), labels_lm.view(-1))\n","            loss = loss_cls + loss_lm\n","\n","            loss_val = loss_lm.item()\n","            losses.append(loss_val)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            pbar.update(1)\n","            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n","    return np.mean(losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFfDno62MCCb","executionInfo":{"status":"aborted","timestamp":1701410429498,"user_tz":-540,"elapsed":3,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[],"source":["config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(config)\n","\n","learning_rate = 5e-5\n","n_epoch = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VkofvdbFMEX1","executionInfo":{"status":"aborted","timestamp":1701410429499,"user_tz":-540,"elapsed":4,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[],"source":["model = BERTPretrain(config)\n","\n","save_pretrain = f\"{data_dir}/save_bert_pretrain.pth\"\n","best_epoch, best_loss = 0, 0\n","if os.path.isfile(save_pretrain):\n","    best_epoch, best_loss = model.bert.load(save_pretrain)\n","    print(f\"load pretrain from: {save_pretrain}, epoch={best_epoch}, loss={best_loss}\")\n","    best_epoch += 1\n","\n","model.to(config.device)\n","\n","criterion_lm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n","criterion_cls = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","losses = []\n","offset = best_epoch\n","for step in range(n_epoch):\n","    epoch = step + offset\n","    if 0 < step:\n","        del train_loader\n","        dataset = PretrainDataSet(vocab, f\"{data_dir}/ratings_train_bert_{epoch % count}.json\")\n","        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pretrin_collate_fn)\n","\n","    loss = train_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader)\n","    losses.append(loss)\n","    model.bert.save(epoch, loss, save_pretrain)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FFSPSlQMn29","executionInfo":{"status":"aborted","timestamp":1701410429499,"user_tz":-540,"elapsed":4,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[],"source":["# data\n","data = {\n","    \"loss\": losses\n","}\n","df = pd.DataFrame(data)\n","display(df)\n","\n","# graph\n","plt.figure(figsize=[12, 4])\n","plt.plot(losses, label=\"loss\")\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"695d036ac30e4713b0e530e7c33cdf4d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04941caeb4f347d5b2c97baa3f9f3c56","IPY_MODEL_135656ae3df74edcabb1671f893c1a36","IPY_MODEL_29254ad115fc452982088336d4d6d751"],"layout":"IPY_MODEL_03c74b5cd7974f0f8b8ccca5c18d96c2"}},"04941caeb4f347d5b2c97baa3f9f3c56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae126c9cc52f48318ea6039bccadfb0b","placeholder":"​","style":"IPY_MODEL_6c6452b3781a43fca41d113b9a6b8bf9","value":"Loading: 100%"}},"135656ae3df74edcabb1671f893c1a36":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd78b5ff04b6414ca2847570f94d712a","max":150001,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93323cd3f75a4c4693af4add00b5aab0","value":150001}},"29254ad115fc452982088336d4d6d751":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_097d23f411844023b7ec1a000c989d5d","placeholder":"​","style":"IPY_MODEL_47b06f4dda5241b8ae164d993f2cf882","value":" 150001/150001 [00:16&lt;00:00, 8053.98it/s]"}},"03c74b5cd7974f0f8b8ccca5c18d96c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae126c9cc52f48318ea6039bccadfb0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c6452b3781a43fca41d113b9a6b8bf9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd78b5ff04b6414ca2847570f94d712a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93323cd3f75a4c4693af4add00b5aab0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"097d23f411844023b7ec1a000c989d5d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47b06f4dda5241b8ae164d993f2cf882":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}